/*
 * This Java source file was generated by the Gradle 'init' task.
 */
package ru.arenadata.mm.consumer;

import java.lang.*;
import java.io.*;
import java.util.*;
import java.util.concurrent.TimeoutException;
import java.time.Duration;

import org.apache.kafka.common.TopicPartition;
import org.apache.kafka.common.utils.Utils;
import org.apache.kafka.clients.consumer.*;
import org.apache.kafka.connect.mirror.*;

public class FailoverConsumer {

private static KafkaConsumer<Long, String> createConsumer(String bootstrap, String group, String topic, Map<TopicPartition, OffsetAndMetadata> newOffsets) {
  Properties props = new Properties();
  props.setProperty(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrap);
  props.setProperty(ConsumerConfig.GROUP_ID_CONFIG, group);
  props.setProperty(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, "org.apache.kafka.common.serialization.StringDeserializer");
  props.setProperty(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, "org.apache.kafka.common.serialization.StringDeserializer");
  props.setProperty("enable.auto.commit", "true");
  props.setProperty("auto.commit.interval.ms", "1000");
  KafkaConsumer<Long, String> consumer = new KafkaConsumer<>(props);
  if (newOffsets.isEmpty())
    consumer.subscribe(Arrays.asList(topic.split("\\s*,\\s*")));
  else{
    consumer.assign(newOffsets.keySet());
    for (Map.Entry<TopicPartition, OffsetAndMetadata> entry : newOffsets.entrySet())
      consumer.seek(entry.getKey(), entry.getValue());
    }
  return consumer;
}

public static Map<String, Object> propsToStringObjectMap(Properties props) {
    Map<String, Object> result = new HashMap<>();
    for (Map.Entry<Object, Object> entry : props.entrySet())
        result.put(entry.getKey().toString(), entry.getValue());
    return result;
}

private static Map<TopicPartition, OffsetAndMetadata> getOffsets(String mmConfigPath,
  String cluster, String group){
  Map<TopicPartition, OffsetAndMetadata> newOffsets = new HashMap<>();
  try{
    Properties mmProps = Utils.loadProps(mmConfigPath);
    Map<String, Object> config = propsToStringObjectMap(mmProps);
    System.out.println(config);
//  MirrorMakerConfig mmConfig = new MirrorMakerConfig(config);
//  MirrorClientConfig mmClientConfig = mmConfig.clientConfig(cluster);
//  MirrorClient mmClient= new MirrorClient(config);
    newOffsets = RemoteClusterUtils.translateOffsets(config, cluster, group, Duration.ofSeconds(300, 1));
    System.out.println("###### newOffsets #####");
    for (Map.Entry<TopicPartition, OffsetAndMetadata> entry : newOffsets.entrySet()) {
      System.out.println("Partition: " + entry.getKey() + "/" + entry.getValue().toString());
    }
  }  catch (InterruptedException ex){
    System.out.println("### ERROR while fetching OffsetAndMetadata ### " + ex.getMessage());
  } catch (TimeoutException et){
    System.out.println("### ERROR while fetching OffsetAndMetadata ### " + et.getMessage());
  } catch (IOException e){
    System.out.println("### ERROR while fetching mm.properties loadProps ### " + e.getMessage());
  }
  return newOffsets;
}

  public static void main(String... args) throws Exception {
    String bootstrap = args[0].toString();
    String topic = args[1].toString();
    String group = args[2].toString();
    String mmConfigPath = args[3].toString();
    String cluster = args[4].toString();

    Map<TopicPartition, OffsetAndMetadata> newOffsets = getOffsets(mmConfigPath, cluster, group);
    KafkaConsumer<Long, String> consumer = createConsumer(bootstrap, group, topic, newOffsets);

    try {
      System.out.println("###### Consuming #####");
      while (true) {
        ConsumerRecords<Long, String> records = consumer.poll(Duration.ofMillis(100));
        for (ConsumerRecord<Long, String> record : records)
        System.out.printf("offset = %d, key = %s, value = %s%n", record.offset(), record.key(), record.value());
      }
    } finally {
      consumer.close();
    }
  }

}
